关于爬虫，需要掌握
1.python语言
2.HTML内容抓取
3.HTML解析（数据清洗）
4.爬虫框架Scrapy


为什么使用python作为爬虫语言：
1.php,特长是web应用，数据吞吐量小，php对多线程，异步支持不好
2.java爬虫生态圈很完善，语法笨重，代码量大，重构的工作量大
3.c/c++效率极高，成本高，代码成型很慢
4.python的模块（库）多，代码简洁，开发效率高，对HTML的解析的支持好



爬虫就是模拟浏览器去访问网页

所以需要程序员自己创建http的head：
1.User-Agent,代理，指的就是哪个浏览器，必须写
2.Accept-Encoding，不能写
3.Cookie,视情况而定



网页抓取，在python有很多库可以用以抓取网页，urllib2
python2中urllib2
python3中urllib.request

urllib2的urlopen方法发送请求，默认User-agent是：Python-urllib/2.7

创建请求头
urllib2.Request类

URL不允许出现中文，如果出现中文需要编码
urllib.urlencode()参数是字典



总结：
urllib
urllib2
都可以接受url请求的模块，功能不同，区别：
urllib只可以接受url，不能创建，但是urllib提供了urlencode方法，用来生成GET请求参数，而urllib2没有，urlunquote()方法可以解码

urllib2默认只支持get和post请求

get方式之前已经讲过，下面一起学习post方式：
之前使用urlopen()或者创建request对象，他们都有一个参数data，data参数就是用于post请求，post请求的数据不在url里
模拟一个post，步骤：
1.找到url
2.创建header
3.创建post请求参数
4.创建request
5.发送请求


opener是urllib2模块中OpenerDirector的对象，该对象的作用就是发送请求，获取响应。我们使用urlopen方法中会默认构建opener对象，默认的opener仅仅只有发送请求的功能，如果需要在发送请求的过程中，需要支持代理，处理cookie等操作时，就需要自定义opener，步骤：
1.根据需求创建相应的handler
2.创建opener,使用urllib2.build_opener()方法创建
3.使用opener的open()方法发送请求

如果说所有的请求都是要采用同一种opener的话，可以使用urllib2.install_opener设置全局的默认opener,这样在使用urlopen方法时就统一使用同一个opener

例，使用代理时，可以使用proxyHandler处理器设置代理
开放代理：
urllib2.ProxyHandler({'http':"host:port"})

需要用户名密码的私密代理:
urllib2.ProxyHandler({'http':"用户名：密码@host:port"})

cookielib模块，实现登陆之后的状态保持
1.需要创建一个对象来获取cookie,使用CookieJar类来保存cookie的值

2.使用cookie对象来创建handler

3.通过handler创建opener

cookie = cookielib.CookieJar()

cookiehandler = urllib2.HTTPCookieProcessor(cookie)

opener = urllib2.build_opener(cookiehandler)














































